# ğŸ§  `DCGAN-MNIST`: Generating Handwritten Digits with GANs

> Part of the **GenAI FastTrack** series â€“ building foundational generative AI capabilities from scratch.

---

## ğŸ“Œ Overview

This project implements a Deep Convolutional GAN (DCGAN) to generate synthetic handwritten digits using the MNIST dataset.  
It integrates real-world GAN training techniques to overcome instability, improve generator learning, and visualize progress during training â€” even on CPU.

---

## ğŸš€ Highlights

- âœ… Generator & Discriminator fully built from scratch using PyTorch
- âœ… Dynamic label smoothing, dropout, and input noise to stabilize training
- âœ… Generator progress saved every 200 steps for **live visual feedback**
- âœ… Fully CPU-compatible for resource-constrained training
- âš ï¸ Training run demonstrated until ~step 400 (partial run)

---

## ğŸ—‚ï¸ Project Structure

```bash
genai_rl_agents/
â”œâ”€â”€ dcgan_mnist.py                # ğŸ”§ Main training script
â”œâ”€â”€ output/                       # ğŸ“¸ Generated samples
â”‚   â”œâ”€â”€ fake_samples_epoch0_step0.png
â”‚   â””â”€â”€ ...
â”œâ”€â”€ data/                         # (excluded) MNIST raw dataset
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md
```
---

## ğŸ“¦ Setup
Install required packages:
```bash
pip install -r requirements.txt
```
---
## â–¶ï¸ Run Training
```bash
python dcgan_mnist.py
```
Training will:
* Print `Loss_D` and `Loss_G` every 100 batches
* Save generated image samples every 200 steps
* Output saved in `output/` folder
> â³ CPU mode is supported (used in this run), but training may take longer.

> âš ï¸ Note: Training on CPU may take 5â€“10Ã— longer than on GPU.
---
## ğŸ“ˆ Sample Results
Here are generated images from an early-stage training run (~step 400):
```markdown
| Step | Generated Image |
|------|-----------------|
| Step 0 | ![](output/fake_samples_epoch0_step0.png) |
| Step 400 | ![](output/fake_samples_epoch0_step400.png) |
```
> âš ï¸ Full training was not completed due to CPU limitations.
> Generator quality improves further beyond ~5 epochs on GPU.
---
## ğŸ§ª Training Tricks Used
| Technique | Purpose |
|------|-----------------|
| Label Smoothing (0.8-1.0) | Prevent overconfident Discriminator |
| Gaussian Noise on Real Inputs | Regularize Discriminator |
| Dropout in D (0.3) | Reduce overfitting |
| Fixed Noise | Consistent image comparison over time |
---
## ğŸ’¡ What I Learned
* How GANs pit two networks (G vs D) in adversarial training
* Common training imbalances and stabilization tricks
* Live-saving of fake image samples for monitoring progress
* How to build and train deep learning models even without GPU
---
## ğŸ“š References
```markdown
- [DCGAN Paper (Radford et al., 2015)](https://arxiv.org/abs/1511.06434)
- [PyTorch DCGAN Tutorial](https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html)
```
---
## ğŸ Next Steps
* Complete full training (5â€“10 epochs on GPU)
* Create animated GIF showing Generator improvement
* Add model checkpoint saving
* Log loss curves over training
* Try Conditional GAN (CGAN) on digit labels
